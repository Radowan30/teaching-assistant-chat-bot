"""Simple chat app example build with FastAPI.

Run with:

    uv run -m pydantic_ai_examples.chat_app
"""

from __future__ import annotations as _annotations

import asyncio
import json
import sqlite3
from collections.abc import AsyncIterator
from concurrent.futures.thread import ThreadPoolExecutor
from contextlib import asynccontextmanager
from dataclasses import dataclass
from datetime import datetime, timezone
from functools import partial
from pathlib import Path
from typing import Annotated, Any, Callable, Literal, TypeVar

import fastapi
import logfire
from fastapi import Depends, Request
from fastapi.responses import FileResponse, Response, StreamingResponse
from typing_extensions import LiteralString, ParamSpec, TypedDict

from pydantic_ai import Agent, ModelRetry, RunContext
from pydantic_ai.exceptions import UnexpectedModelBehavior
from pydantic_ai.messages import (
    ModelMessage,
    ModelMessagesTypeAdapter,
    ModelRequest,
    ModelResponse,
    TextPart,
    UserPromptPart,
)
import os
from datetime import datetime
from httpx import AsyncClient
from dotenv import load_dotenv
from pydantic_ai.models.openai import OpenAIModel
from fastapi import HTTPException

load_dotenv()
llm = os.getenv('LLM_MODEL', 'gpt-4o')

model = OpenAIModel(llm)


# 'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured
logfire.configure(send_to_logfire='if-token-present')


@dataclass
class Deps:
    client: AsyncClient
    brave_api_key: str | None



agent = Agent( model,
    system_prompt=f'You are a helpful teaching assistant. When needed you can also be an expert at researching the web to answer user questions. You can also generate animation videos using the Manim API when the user passes a prompt that contains the keyword @video (do not invoke the generate_manim_video() tool unless this keyword is provided by the user). If the user wants a video or animation to be made and they do not mention the @video keyword, then tell them that they should pass a message that starts with the keyword, followed by the animation description. Example: @video make a circle going from left to right. The current date is: {datetime.now().strftime("%Y-%m-%d")}',
    deps_type=Deps,
    retries=2)

#Web search tool
@agent.tool
async def search_web(
    ctx: RunContext[Deps], web_query: str
) -> str:
    """Search the web given a query defined to answer the user's question.

    Args:
        ctx: The context.
        web_query: The query for the web search.

    Returns:
        str: The search results as a formatted string.
    """
    if ctx.deps.brave_api_key is None:
        return "This is a test web search result. Please provide a Brave API key to get real search results."

    headers = {
        'X-Subscription-Token': ctx.deps.brave_api_key,
        'Accept': 'application/json',
    }
    
    with logfire.span('calling Brave search API', query=web_query) as span:
        r = await ctx.deps.client.get(
            'https://api.search.brave.com/res/v1/web/search',
            params={
                'q': web_query,
                'count': 5,
                'text_decorations': True,
                'search_lang': 'en'
            },
            headers=headers
        )
        r.raise_for_status()
        data = r.json()
        span.set_attribute('response', data)

    results = []
    
    # Add web results in a nice formatted way
    web_results = data.get('web', {}).get('results', [])
    for item in web_results[:3]:
        title = item.get('title', '')
        description = item.get('description', '')
        url = item.get('url', '')
        if title and description:
            results.append(f"Title: {title}\nSummary: {description}\nSource: {url}\n")

    return "\n".join(results) if results else "No results found for the query."

#Manim video generation tool
@agent.tool
async def generate_manim_video(ctx: RunContext, prompt: str) -> str:
    """
    Take the user's prompt, send it to the Manim API to generate code and render a video,
    and return the video URL.
    """
    manim_api_base = "http://127.0.0.1:8080"  # Adjust if needed

    async with AsyncClient() as client:
        try:
            # Step 1: Generate Manim code from the prompt
            code_generation_payload = {"prompt": prompt, "model": "gpt-4o"}
            code_response = await client.post(
                f"{manim_api_base}/v1/code/generation",
                json=code_generation_payload,
                timeout=60  # Adjust timeout as necessary
            )
            code_response.raise_for_status()
            code_data = code_response.json()
            generated_code = code_data.get("code")
            if not generated_code:
                raise HTTPException(status_code=500, detail="No code was generated by the Manim API.")

            # Remove markdown formatting if present
            import re
            generated_code = re.sub(r"^```(?:python)?", "", generated_code)
            generated_code = re.sub(r"```$", "", generated_code).strip()
            generated_code = generated_code.replace("```", "").strip()

            # Step 2: Render the video using the generated code.
            # Here, you need to provide additional parameters required by your video rendering endpoint.
            video_render_payload = {
                "code": generated_code,
                "file_name": "scene_temp",        # or generate a unique name
                "file_class": "GenScene",          # assuming we always use GenScene unless specified
                "user_id": "some_user_id",         # You can pass the current user id if available
                "project_name": "default_project", # Adjust as needed
                "iteration": 1,
                "aspect_ratio": "16:9",
                "stream": False,  # For simplicity, get a final result rather than a stream
            }
            video_response = await client.post(
                f"{manim_api_base}/v1/video/rendering",
                json=video_render_payload,
                timeout=3000000  # video rendering might take longer
            )
            video_response.raise_for_status()
            video_data = video_response.json()
            video_url = video_data.get("video_url")
            if not video_url:
                raise HTTPException(status_code=500, detail="Video rendering failed to return a URL.")
            return video_url

        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Error generating video: {str(e)}")
        


THIS_DIR = Path(__file__).parent


@asynccontextmanager
async def lifespan(_app: fastapi.FastAPI):
    async with Database.connect() as db:
        yield {'db': db}


app = fastapi.FastAPI(lifespan=lifespan)
logfire.instrument_fastapi(app)


@app.get('/')
async def index() -> FileResponse:
    return FileResponse((THIS_DIR / 'chat_app.html'), media_type='text/html')


@app.get('/chat_app.ts')
async def main_ts() -> FileResponse:
    """Get the raw typescript code, it's compiled in the browser, forgive me."""
    return FileResponse((THIS_DIR / 'chat_app.ts'), media_type='text/plain')


async def get_db(request: Request) -> Database:
    return request.state.db


@app.get('/chat/')
async def get_chat(database: Database = Depends(get_db)) -> Response:
    msgs = await database.get_messages()
    return Response(
        b'\n'.join(json.dumps(to_chat_message(m)).encode('utf-8') for m in msgs),
        media_type='text/plain',
    )


class ChatMessage(TypedDict):
    """Format of messages sent to the browser."""

    role: Literal['user', 'model', 'video', 'progress']
    timestamp: str
    content: str

#original
# def to_chat_message(m: ModelMessage) -> ChatMessage:
#     first_part = m.parts[0]
#     if isinstance(m, ModelRequest):
#         if isinstance(first_part, UserPromptPart):
#             return {
#                 'role': 'user',
#                 'timestamp': first_part.timestamp.isoformat(),
#                 'content': first_part.content,
#             }
#     elif isinstance(m, ModelResponse):
#         if isinstance(first_part, TextPart):
#             return {
#                 'role': 'model',
#                 'timestamp': m.timestamp.isoformat(),
#                 'content': first_part.content,
#             }
#     raise UnexpectedModelBehavior(f'Unexpected message type for chat app: {m}')



#Clickable link version
# def to_chat_message(m: ModelMessage) -> ChatMessage:
#     if isinstance(m, ModelRequest):
#         # Look for a user prompt in the parts.
#         for part in m.parts:
#             if isinstance(part, UserPromptPart):
#                 return {
#                     'role': 'user',
#                     'timestamp': part.timestamp.isoformat(),
#                     'content': part.content,
#                 }
#         return {
#             'role': 'user',
#             'timestamp': m.parts[0].timestamp.isoformat(),
#             'content': m.parts[0].content,
#         }
#     elif isinstance(m, ModelResponse):
#         first_part = m.parts[0]
#         if isinstance(first_part, TextPart):
#             content = first_part.content.strip()
#             # Try JSON parsing
#             try:
#                 data = json.loads(content)
#                 if isinstance(data, dict) and "video_url" in data:
#                     return {
#                         'role': 'video',
#                         'timestamp': m.timestamp.isoformat(),
#                         'content': data["video_url"],
#                     }
#             except Exception:
#                 pass

#             # Check for explicit marker
#             if content.startswith("VIDEO:"):
#                 video_url = content[len("VIDEO:"):].strip()
#                 return {
#                     'role': 'video',
#                     'timestamp': m.timestamp.isoformat(),
#                     'content': video_url,
#                 }
            
#             # Check if the content is a plain URL ending in .mp4
#             if content.startswith("http") and content.endswith(".mp4"):
#                 return {
#                     'role': 'video',
#                     'timestamp': m.timestamp.isoformat(),
#                     'content': content,
#                 }

#             # Otherwise, treat it as a normal text message
#             return {
#                 'role': 'model',
#                 'timestamp': m.timestamp.isoformat(),
#                 'content': content,
#             }
#     raise UnexpectedModelBehavior(f'Unexpected message type for chat app: {m}')

#video generation version
def to_chat_message(m: ModelMessage) -> ChatMessage:
    if isinstance(m, ModelRequest):
        # Look for a user prompt in the parts.
        for part in m.parts:
            if isinstance(part, UserPromptPart):
                return {
                    'role': 'user',
                    'timestamp': part.timestamp.isoformat(),
                    'content': part.content,
                }
        # Fallback if no explicit user prompt is found.
        return {
            'role': 'user',
            'timestamp': m.parts[0].timestamp.isoformat(),
            'content': m.parts[0].content,
        }
    elif isinstance(m, ModelResponse):
        first_part = m.parts[0]
        if isinstance(first_part, TextPart):
            content = first_part.content.strip()
            # Try to parse content as JSON.
            try:
                data = json.loads(content)
                if isinstance(data, dict) and "video_url" in data:
                    return {
                        'role': 'video',
                        'timestamp': m.timestamp.isoformat(),
                        'content': data["video_url"],
                    }
            except Exception:
                pass

            # Check for explicit "VIDEO:" marker.
            if content.startswith("VIDEO:"):
                video_url = content[len("VIDEO:"):].strip()
                return {
                    'role': 'video',
                    'timestamp': m.timestamp.isoformat(),
                    'content': video_url,
                }

            # Check if the content is a plain video URL.
            # Use lower() and strip() to catch extra spaces or different cases.
            if content.lower().startswith("http") and content.lower().endswith(".mp4"):
                return {
                    'role': 'video',
                    'timestamp': m.timestamp.isoformat(),
                    'content': content,
                }
            
            # Otherwise, treat as a normal text response.
            return {
                'role': 'model',
                'timestamp': m.timestamp.isoformat(),
                'content': content,
            }
    raise UnexpectedModelBehavior(f'Unexpected message type for chat app: {m}')


#video generation with progress bar version
# def to_chat_message(m: ModelMessage) -> ChatMessage:
#     if isinstance(m, ModelRequest):
#         # Look for a user prompt in the parts.
#         for part in m.parts:
#             if isinstance(part, UserPromptPart):
#                 return {
#                     'role': 'user',
#                     'timestamp': part.timestamp.isoformat(),
#                     'content': part.content,
#                 }
#         return {
#             'role': 'user',
#             'timestamp': m.parts[0].timestamp.isoformat(),
#             'content': m.parts[0].content,
#         }
#     elif isinstance(m, ModelResponse):
#         first_part = m.parts[0]
#         if isinstance(first_part, TextPart):
#             content = first_part.content.strip()
#             # Try to parse the content as JSON.
#             try:
#                 data = json.loads(content)
#                 # If it's a progress update, return role "progress".
#                 if isinstance(data, dict) and "animationIndex" in data and "percentage" in data:
#                     return {
#                         'role': 'progress',
#                         'timestamp': m.timestamp.isoformat(),
#                         'content': content,
#                     }
#                 # If it contains a video URL, return as video.
#                 if isinstance(data, dict) and "video_url" in data:
#                     return {
#                         'role': 'video',
#                         'timestamp': m.timestamp.isoformat(),
#                         'content': data["video_url"],
#                     }
#             except Exception:
#                 pass

#             # Check for an explicit marker.
#             if content.startswith("VIDEO:"):
#                 video_url = content[len("VIDEO:"):].strip()
#                 return {
#                     'role': 'video',
#                     'timestamp': m.timestamp.isoformat(),
#                     'content': video_url,
#                 }
#             # Otherwise, treat it as a normal text response.
#             return {
#                 'role': 'model',
#                 'timestamp': m.timestamp.isoformat(),
#                 'content': content,
#             }
#     raise UnexpectedModelBehavior(f'Unexpected message type for chat app: {m}')


# @app.post('/chat/')
# async def post_chat(
#     prompt: Annotated[str, fastapi.Form()], database: Database = Depends(get_db)
# ) -> StreamingResponse:
#     async def stream_messages():
#         """Streams new line delimited JSON `Message`s to the client."""
#         # stream the user prompt so that can be displayed straight away
#         yield (
#             json.dumps(
#                 {
#                     'role': 'user',
#                     'timestamp': datetime.now(tz=timezone.utc).isoformat(),
#                     'content': prompt,
#                 }
#             ).encode('utf-8')
#             + b'\n'
#         )

#         async with AsyncClient() as client:
#             # get the chat history so far to pass as context to the agent
#             messages = await database.get_messages()
#             brave_api_key = os.getenv('BRAVE_API_KEY', None)
#             deps = Deps(client=client, brave_api_key=brave_api_key)

#             # run the agent with the user prompt and the chat history
#             async with agent.run_stream(prompt, message_history=messages, deps=deps) as result:
#                 async for text in result.stream(debounce_by=0.01):
#                     # text here is a `str` and the frontend wants
#                     # JSON encoded ModelResponse, so we create one
#                     m = ModelResponse(parts=[TextPart(text)], timestamp=result.timestamp())
#                     yield json.dumps(to_chat_message(m)).encode('utf-8') + b'\n'

#             # add new messages (e.g. the user prompt and the agent response in this case) to the database
#             await database.add_messages(result.new_messages_json())

#     return StreamingResponse(stream_messages(), media_type='text/plain')


@app.post('/chat/')
async def post_chat(
    prompt: Annotated[str, fastapi.Form()], database: Database = Depends(get_db)
) -> StreamingResponse:
    async def stream_messages():
        # Stream the user prompt immediately.
        yield (
            json.dumps({
                'role': 'user',
                'timestamp': datetime.now(tz=timezone.utc).isoformat(),
                'content': prompt,
            }).encode('utf-8')
            + b'\n'
        )

        async with AsyncClient() as client:
            messages = await database.get_messages()
            brave_api_key = os.getenv('BRAVE_API_KEY', None)
            deps = Deps(client=client, brave_api_key=brave_api_key)

            # Decide if this is a video generation request.
            is_video_request = any(
                keyword in prompt.lower() 
                for keyword in ["@video"]
            )

            if is_video_request:
                # Accumulate the entire response.
                accumulated_text = ""
                async with agent.run_stream(prompt, message_history=messages, deps=deps) as result:
                    async for text in result.stream(debounce_by=0.01):
                        accumulated_text += text
                    # Extract a clean video URL (assumes URL ends with .mp4).
                    import re
                    match = re.search(r'(http://[^\s\)]+\.mp4)', accumulated_text)
                    if match:
                        clean_video_url = match.group(1)
                    else:
                        clean_video_url = accumulated_text
                    final_msg = ModelResponse(
                        parts=[TextPart(clean_video_url)],
                        timestamp=result.timestamp()
                    )
                    yield json.dumps(to_chat_message(final_msg)).encode('utf-8') + b'\n'
            else:
                # Stream messages normally.
                async with agent.run_stream(prompt, message_history=messages, deps=deps) as result:
                    async for text in result.stream(debounce_by=0.01):
                        m = ModelResponse(parts=[TextPart(text)], timestamp=result.timestamp())
                        yield json.dumps(to_chat_message(m)).encode('utf-8') + b'\n'

            await database.add_messages(result.new_messages_json())

    return StreamingResponse(stream_messages(), media_type='text/plain')


P = ParamSpec('P')
R = TypeVar('R')


@dataclass
class Database:
    """Rudimentary database to store chat messages in SQLite.

    The SQLite standard library package is synchronous, so we
    use a thread pool executor to run queries asynchronously.
    """

    con: sqlite3.Connection
    _loop: asyncio.AbstractEventLoop
    _executor: ThreadPoolExecutor

    @classmethod
    @asynccontextmanager
    async def connect(
        cls, file: Path = THIS_DIR / '.chat_app_messages.sqlite'
    ) -> AsyncIterator[Database]:
        with logfire.span('connect to DB'):
            loop = asyncio.get_event_loop()
            executor = ThreadPoolExecutor(max_workers=1)
            con = await loop.run_in_executor(executor, cls._connect, file)
            slf = cls(con, loop, executor)
        try:
            yield slf
        finally:
            await slf._asyncify(con.close)

    @staticmethod
    def _connect(file: Path) -> sqlite3.Connection:
        con = sqlite3.connect(str(file))
        con = logfire.instrument_sqlite3(con)
        cur = con.cursor()
        cur.execute(
            'CREATE TABLE IF NOT EXISTS messages (id INT PRIMARY KEY, message_list TEXT);'
        )
        con.commit()
        return con

    async def add_messages(self, messages: bytes):
        await self._asyncify(
            self._execute,
            'INSERT INTO messages (message_list) VALUES (?);',
            messages,
            commit=True,
        )
        await self._asyncify(self.con.commit)

    async def get_messages(self) -> list[ModelMessage]:
        c = await self._asyncify(
            self._execute, 'SELECT message_list FROM messages order by id'
        )
        rows = await self._asyncify(c.fetchall)
        messages: list[ModelMessage] = []
        for row in rows:
            messages.extend(ModelMessagesTypeAdapter.validate_json(row[0]))
        return messages

    def _execute(
        self, sql: LiteralString, *args: Any, commit: bool = False
    ) -> sqlite3.Cursor:
        cur = self.con.cursor()
        cur.execute(sql, args)
        if commit:
            self.con.commit()
        return cur

    async def _asyncify(
        self, func: Callable[P, R], *args: P.args, **kwargs: P.kwargs
    ) -> R:
        return await self._loop.run_in_executor(  # type: ignore
            self._executor,
            partial(func, **kwargs),
            *args,  # type: ignore
        )


if __name__ == '__main__':
    import uvicorn

    uvicorn.run(
        'chat_app:app', reload=True, reload_dirs=[str(THIS_DIR)]
    )